open(connection)
# Streaming the data as text
output <- readLines(connection)
# Closing the streaming connection
close(connection)
# Pulling out the data of interest
output_length <- length(output)
data_string <- str_detect(output, "How to")
data_position <- match(TRUE, data_string)
tail_position <- output_length - data_position + 1
daymet_data <- tail(output, tail_position)
daymet_data
output
data_string <- str_detect(output, "year,yday")
data_position <- match(TRUE, data_string)
tail_position <- output_length - data_position + 1
daymet_data <- tail(output, tail_position)
daymet_data
# Splitting the data strings
str_split(daymet_data, ",")
# Splitting the data strings
x <- str_split(daymet_data, ",")
View(x)
# Splitting the data strings
daymet_data <- as_tibble(daymet_data)
rm(x)
View(daymet_data)
separate_wider_delim(daymet_data, delim = ",")
separate_wider_delim(daymet_data, cols = value, delim = ",")
separate_wider_delim(daymet_data, cols = value, names = NULL, delim = ",")
separate_wider_delim(daymet_data, cols = value, names = Var, delim = ",")
separate_wider_delim(daymet_data, cols = value, names = "Var", delim = ",")
View(daymet_data)
separate_wider_delim(daymet_data, cols = value, names = c("Var1", "Var2", "Var3", "Var4", "Var5", "Var6", "Var7", "Var8", "Var9"), delim = ",")
daymet_data <- tail(output, tail_position)
# Splitting the data strings
daymet_data <- as_tibble(daymet_data)
View(daymet_data)
x <- str_split(daymet_data, ",")
View(x)
daymet_data <- tail(output, tail_position)
# Splitting the data strings
x <- str_split(daymet_data, ",")
daymet_data <- as_tibble(daymet_data)
View(x)
length(x)
length(x[[1]])
daymet_data <- tail(output, tail_position)
daymet_data
# Splitting the data strings
num_values <- str_split(daymet_data[1], ",")
rm(x)
View(num_values)
# Splitting the data strings
num_values <- length(str_split(daymet_data[1], ","))
# Splitting the data strings
num_values <- length(str_split(daymet_data[1], ",")[[1]])
daymet_data <- tail(output, tail_position)
daymet_data
str_split(daymet_data[1], ",")
daymet_data[[1]]
str_split(daymet_data[[1]], ",")
length(str_split(daymet_data[[1]], ","))
daymet_data[[1]]
daymet_data[[1]]
str_split(daymet_data[[1]], ",")
str_split(daymet_data[[1]], ",", simplify = TRUE)
length(str_split(daymet_data[[1]], ",", simplify = TRUE))
str_split(daymet_data, ",", simplify = TRUE)
str_split(daymet_data, ",", simplify = TRUE)
x <- str_split(daymet_data, ",", simplify = TRUE)
View(x)
x <- as_tibble(x)
View(x)
row_to_names(x)
row_to_names(x)
library(janitor)
row_to_names(x)
?row_to_names
row_to_names(x, row_number = 1)
rm(x)
daymet_data <- tail(output, tail_position)
# Splitting the data strings
daymet_data <- str_split(daymet_data, ",", simplify = TRUE)
View(daymet_data)
daymet_data <- as_tibble(daymet_data)
View(daymet_data)
daymet_data <- row_to_names(daymet_data, row_number = 1)
View(daymet_data)
str(daymet_data)
daymet_data <- daymet_data |>
mutate_if(is.character, as.numeric)
str(daymet_data)
View(daymet_data)
View(daymet_data)
# Setting a URL - this can be customized with Latitude, Longitude, Variables of Interest, Start Date, and End Date
url <- 'https://daymet.ornl.gov/single-pixel/api/data?lat=35.9621&lon=-84.2916&vars=dayl,prcp,srad,swe,tmax,tmin,vp&start=2020-01-01&end=2020-01-01'
# Alternatively, streaming the data directly into R
# Opening a streaming connection
connection <- curl(url)
open(connection)
# Streaming the data as text
output <- readLines(connection)
# Closing the streaming connection
close(connection)
# Pulling out the data of interest
output_length <- length(output)
data_string <- str_detect(output, "year,yday")
data_position <- match(TRUE, data_string)
tail_position <- output_length - data_position + 1
daymet_data <- tail(output, tail_position)
# Splitting the data strings
daymet_data <- str_split(daymet_data, ",", simplify = TRUE)
# Putting data into tibble
daymet_data <- as_tibble(daymet_data)
daymet_data <- row_to_names(daymet_data, row_number = 1)
# Converting data to numeric
daymet_data <- daymet_data |>
mutate_if(is.character, as.numeric)
View(daymet_data)
# Let's try getting Daymet data a different way, downloading it directly from the web
# Here's some documentation: https://daymet.ornl.gov/web_services#single
# Loading necessary packages
library(curl)
library(tidyverse)
library(janitor)
# Writing a function to stream Daymet data from a URL
# Function arguments:
# Latitude as string
lat <- ""
# Longitude as string
lon <- ""
# Comma-separated Daymet variables of interest as string, a selection of: dayl,prcp,srad,swe,tmax,tmin,vp
vars <- ""
# Start Date as string, in format YYYY-MM-DD
start <- ""
# End Date as string, in format YYYY-MM-DD
end <- ""
# Building URL
str_glue('https://daymet.ornl.gov/single-pixel/api/data?lat=',
lat,
'&lon=',
lon,
'&vars=',
vars,
'&start=',
start,
'&end=',
end,
sep = "")
# Building URL
url <- str_glue('https://daymet.ornl.gov/single-pixel/api/data?lat=',
lat,
'&lon=',
lon,
'&vars=',
vars,
'&start=',
start,
'&end=',
end,
sep = "")
url
# Function arguments:
# Latitude as string
lat <- "35.9621"
# Longitude as string
lon <- "-84.2916"
# Comma-separated Daymet variables of interest as string, a selection of: dayl,prcp,srad,swe,tmax,tmin,vp
vars <- "tmax"
# Start Date as string, in format YYYY-MM-DD
start <- "2020-01-01"
# End Date as string, in format YYYY-MM-DD
end <- "2020-01-01"
# Building URL
url <- str_glue('https://daymet.ornl.gov/single-pixel/api/data?lat=',
lat,
'&lon=',
lon,
'&vars=',
vars,
'&start=',
start,
'&end=',
end,
sep = "")
url
# Opening a streaming connection
connection <- curl(url)
open(connection)
# Streaming the data as text
output <- readLines(connection)
# Closing the streaming connection
close(connection)
# Pulling out the data of interest
output_length <- length(output)
data_string <- str_detect(output, "year,yday")
data_position <- match(TRUE, data_string)
tail_position <- output_length - data_position + 1
daymet_data <- tail(output, tail_position)
# Splitting the data strings
daymet_data <- str_split(daymet_data, ",", simplify = TRUE)
# Putting data into tibble
daymet_data <- as_tibble(daymet_data)
daymet_data <- row_to_names(daymet_data, row_number = 1)
# Converting data to numeric
daymet_data <- daymet_data |>
mutate_if(is.character, as.numeric)
View(daymet_data)
# Building URL
url <- str_glue('https://daymet.ornl.gov/single-pixel/api/data?lat=',
lat,
'&lon=',
lon,
'&vars=',
vars,
'&start=',
start,
'&end=',
end,
sep = "")
# Opening a streaming connection
connection <- curl(url)
open(connection)
# Streaming the data as text
output <- readLines(connection)
# Closing the streaming connection
close(connection)
output
# Pulling out the data of interest
output_length <- length(output)
data_string <- str_detect(output, "year,yday")
data_position <- match(TRUE, data_string)
tail_position <- output_length - data_position + 1
daymet_data <- tail(output, tail_position)
daymet_data
# Splitting the data strings
daymet_data <- str_split(daymet_data, ",", simplify = TRUE)
View(daymet_data)
# Putting data into tibble
daymet_data <- as_tibble(daymet_data, .name_repair = 'unique')
View(daymet_data)
daymet_data <- row_to_names(daymet_data, row_number = 1)
View(daymet_data)
# Converting data to numeric
daymet_data <- daymet_data |>
mutate_if(is.character, as.numeric)
View(daymet_data)
daymet_download <- function(lat, lon, vars, start, end){
# Building URL
url <- str_glue('https://daymet.ornl.gov/single-pixel/api/data?',
'lat=', lat,
'&lon=', lon,
'&vars=', vars,
'&start=', start,
'&end=', end,
sep = "")
# Opening a streaming connection
connection <- curl(url)
open(connection)
# Streaming the data as text
output <- readLines(connection)
# Closing the streaming connection
close(connection)
# Pulling out the data of interest
output_length <- length(output)
data_string <- str_detect(output, "year,yday")
data_position <- match(TRUE, data_string)
tail_position <- output_length - data_position + 1
daymet_data <- tail(output, tail_position)
# Splitting the data strings
daymet_data <- str_split(daymet_data, ",", simplify = TRUE)
# Putting data into tibble
daymet_data <- as_tibble(daymet_data, .name_repair = 'unique')
daymet_data <- row_to_names(daymet_data, row_number = 1)
# Converting data to numeric
daymet_data <- daymet_data |>
mutate_if(is.character, as.numeric)
}
# Latitude as string
lat <- "35.9621"
# Longitude as string
lon <- "-84.2916"
# Comma-separated Daymet variables of interest as string, a selection of: dayl,prcp,srad,swe,tmax,tmin,vp
vars <- "tmax"
# Start Date as string, in format YYYY-MM-DD
start <- "2020-01-01"
# End Date as string, in format YYYY-MM-DD
end <- "2020-01-01"
daymet_data <- daymet_download(lat, lon, vars, start, end)
View(daymet_data)
# Let's try getting Daymet data a different way, downloading it directly from the web
# Here's some documentation: https://daymet.ornl.gov/web_services#single
# Loading necessary packages
library(curl)
library(tidyverse)
library(janitor)
daymet_download <- function(lat, lon, vars, start, end){
# Building URL
url <- str_glue('https://daymet.ornl.gov/single-pixel/api/data?',
'lat=', lat,
'&lon=', lon,
'&vars=', vars,
'&start=', start,
'&end=', end,
sep = "")
# Opening a streaming connection
connection <- curl(url)
open(connection)
# Streaming the data as text
output <- readLines(connection)
# Closing the streaming connection
close(connection)
# Pulling out the data of interest
output_length <- length(output)
data_string <- str_detect(output, "year,yday")
data_position <- match(TRUE, data_string)
tail_position <- output_length - data_position + 1
daymet_data <- tail(output, tail_position)
# Splitting the data strings
daymet_data <- str_split(daymet_data, ",", simplify = TRUE)
# Putting data into tibble
daymet_data <- as_tibble(daymet_data, .name_repair = 'unique')
daymet_data <- row_to_names(daymet_data, row_number = 1)
# Converting data to numeric
daymet_data <- daymet_data |>
mutate_if(is.character, as.numeric)
}
# Running the function with specified arguments
lat <- "35.9621"
lon <- "-84.2916"
vars <- "tmax"
start <- "2020-01-01"
end <- "2020-01-01"
end <- "2020-01-02"
daymet_data <- daymet_download(lat, lon, vars, start, end)
View(daymet_data)
View(daymet_data)
# Loading necessary packages
library(daymetr)
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
x <- download_daymet_tiles(location = c(36.0133,-84.2625),
tiles = NULL,
start = 1980,
end = 1980,
param = "tmax")
x
tempdir()
getwd()
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
download_daymet_tiles(location = c(36.0133,-84.2625),
tiles = NULL,
start = 1980,
end = 1980,
param = "tmax",
path = "C:/Users/benba/Documents/Defusing Disasters/DeGAUSS/DeGAUSS")
rm(x)
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
download_daymet_tiles(location = c(36.0133,-84.2625),
tiles = NULL,
start = 1980,
end = 1981,
param = "tmax",
path = "C:/Users/benba/Documents/Defusing Disasters/DeGAUSS/DeGAUSS")
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
download_daymet_tiles(location = c(36.0133,-84.2625),
tiles = NULL,
start = 1980,
end = 1981,
param = "tmax",
path = "C:/Users/benba/Documents/Defusing Disasters/DeGAUSS/DeGAUSS/data")
# Loading necessary packages
library(daymetr)
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
download_daymet_tiles(location = c(36.0133,-84.2625),
tiles = NULL,
start = 1980,
end = 1981,
param = "tmax",
path = "C:/Users/benba/Documents/Defusing Disasters/DeGAUSS/DeGAUSS")
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
x <- download_daymet_tiles(location = c(36.0133,-84.2625),
tiles = NULL,
start = 1980,
end = 1981,
param = "tmax",
path = "C:/Users/benba/Documents/Defusing Disasters/DeGAUSS/DeGAUSS")
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
download_daymet_tiles(location = c(36.0133,-84.2625),
tiles = NULL,
start = 1980,
end = 1981,
param = "tmax")
x <- tempdir()
x
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
download_daymet_tiles(location = c(36.0133,-84.2625),
tiles = NULL,
start = 2000,
end = 2001,
param = "tmax")
rm(x)
getwd()
#########
# ON SECOND THOUGHT, I'M NOT SURE IF THE DAYMET API IS HIPAA COMPLIANT. I REACHED OUT TO THEM TO ASK.
# EVERYTHING ABOVE USES THE DAYMET API (FOR SINGLE PIXEL EXTRACTION).
# IF IT IS COMPLIANT, THEN GREAT. IF IT IS NOT, THEN WE WILL NEED TO DOWNLOAD DAYMET DATA THAT SPANS CHICAGO (AND WHEREVER ELSE PATIENTS MIGHT BE).
# WE COULD PROBABLY USE THE DAYMETR PACKAGE FOR THAT SINCE LOCATIONS AREN'T SPECIFIC, OR BUILD OUR OWN TOOL. AGAIN, DAYMETR WOULD BE DOWNLOADING DATA FOR AN ENTIRE YEAR AT MINIMUM.
# THEN WE'D NEED TO EXTRACT DATA FROM THE NETCDF, AND LINK IN THE PATIENT LON/LAT COORDINATES TO THE DAYMET TILES, ALL INTERNALLY.
#########
download_daymet_ncss(location = c(36.61,-85.37,33.57,-81.29),
start = 1980,
end = 1980,
param = "tmin",
silent = FALSE,
path = "C:/Users/benba/Documents/Defusing Disasters/DeGAUSS/DeGAUSS")
library(ncdf4)
daymet_data <- nc_open("tmin_daily_1980_ncss.nc")
attributes(daymet_data$var)
attributes(daymet_data$dim)
View(daymet_data)
attributes(daymet_data$var)
lat <- ncvar_get(gridmet_data, "lat")
lat <- ncvar_get(daymet_data, "lat")
lon <- ncvar_get(daymet_data, "lon")
View(lon)
View(lat)
tmin <- ncvar_get(daymet_data, "tmin")
head(tmin)
attributes(daymet_data$var)
x <- ncvar_get(daymet_data, "lambert_conformal_conic")
x
attributes(daymet_data$dim)
daymet_data$dim$time
daymet_data$dim$y
daymet_data$dim$x
View(lon)
417*390
162630*365
162630
162630*365
dim(lat)
dim(tmin)
dim(tmin) # Tmin for every lat/long, and every day of the year
tmin[1, 1, 1]
tmin[1, 1, ]
View(lat)
View(lon)
min(lat)
max(lat)
min(long)
min(lon)
max(lon)
tile_outlines
x <- tile_outlines
View(x)
x[1]
x <- as_tibble(x)
x <- tidyverse::as_tibble(x)
x <- as.data.frame(x)
x[1]
x[,1]
x[1,]
x[1,]$geometry
